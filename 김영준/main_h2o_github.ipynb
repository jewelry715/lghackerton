{"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"J5wsuM41pHiQ"}},{"cell_type":"code","metadata":{"id":"O63BE2cIol4I"},"source":["import os\n","os.environ['PYTHONHASHSEED'] = str(42)\n","\n","import sys\n","import shutil\n","import copy\n","import pickle\n","import random as rnd\n","\n","import numpy as np\n","from numpy import array, nan, random as np_rnd, where, dot\n","import pandas as pd\n","from pandas import DataFrame as dataframe, Series as series, isna, read_csv\n","\n","from sklearn.model_selection import train_test_split as tts, KFold, StratifiedKFold, GroupKFold, GroupShuffleSplit, StratifiedGroupKFold\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, KBinsDiscretizer, MultiLabelBinarizer\n","from sklearn import metrics\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","import h2o\n","\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.width', 1000)\n","pd.set_option('max_colwidth', 200)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Define utility functions"],"metadata":{"id":"NYRbodwnpRPr"}},{"cell_type":"code","source":["# ===== utility functions =====\n","# label encoding for categorical column with excepting na value\n","def seed_everything(seed=42):\n","    # python random module\n","    rnd.seed(seed)\n","    # numpy random\n","    np_rnd.seed(seed)\n","    # tf random\n","    try:\n","        tf_rnd.set_seed(seed)\n","    except:\n","        pass\n","    # RAPIDS random\n","    try:\n","        cp.random.seed(seed)\n","    except:\n","        pass\n","    # pytorch random\n","    try:\n","        torch.manual_seed(seed)\n","    except:\n","        pass\n","def which(bool_list):\n","    return where(bool_list)[0]\n","def easyIO(x=None, path=None, op=\"r\"):\n","    tmp = None\n","    if op == \"r\":\n","        with open(path, \"rb\") as f:\n","            tmp = pickle.load(f)\n","        return tmp\n","    elif op == \"w\":\n","        with open(path, \"wb\") as f:\n","            pickle.dump(x, f)\n","    else:\n","        print(\"Unknown operation type\")\n","def diff(first, second):\n","    second = set(second)\n","    return [item for item in first if item not in second]\n","def findIdx(data_x, col_names):\n","    return [int(i) for i, j in enumerate(data_x) if j in col_names]\n","def orderElems(for_order, using_ref):\n","    return [i for i in using_ref if i in for_order]\n","# concatenate by row\n","def cbr(df1, df2):\n","    if type(df1) == series:\n","        tmp_concat = series(pd.concat([dataframe(df1), dataframe(df2)], axis=0, ignore_index=True).iloc[:,0])\n","        tmp_concat.reset_index(drop=True, inplace=True)\n","    elif type(df1) == dataframe:\n","        tmp_concat = pd.concat([df1, df2], axis=0, ignore_index=True)\n","        tmp_concat.reset_index(drop=True, inplace=True)\n","    elif type(df1) == np.ndarray:\n","        tmp_concat = np.concatenate([df1, df2], axis=0)\n","    else:\n","        print(\"Unknown Type: return 1st argument\")\n","        tmp_concat = df1\n","    return tmp_concat\n","def change_width(ax, new_value):\n","    for patch in ax.patches :\n","        current_width = patch.get_width()\n","        adj_value = current_width - new_value\n","        # we change the bar width\n","        patch.set_width(new_value)\n","        # we recenter the bar\n","        patch.set_x(patch.get_x() + adj_value * .5)\n","def week_of_month(date):\n","    month = date.month\n","    week = 0\n","    while date.month == month:\n","        week += 1\n","        date -= timedelta(days=7)\n","    return week\n","def getSeason(date):\n","    month = date.month\n","    if month in [3, 4, 5]:\n","        return \"Spring\"\n","    elif month in [6, 7, 8]:\n","        return \"Summer\"\n","    elif month in [9, 10, 11]:\n","        return \"Fall\"\n","    else:\n","        return \"Winter\"\n","def createFolder(directory):\n","    try:\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","    except OSError:\n","        print('Error: Creating directory. ' + directory)\n","def sigmoid(x):\n","    return 1/(1 + np.exp(-x))\n","def dispPerformance(result_dic):\n","    perf_table = dataframe()\n","    index_names = []\n","    for k, v in result_dic.items():\n","        index_names.append(k)\n","        perf_table = pd.concat([perf_table, series(v[\"performance\"]).to_frame().T], ignore_index=True, axis=0)\n","    perf_table.index = index_names\n","    perf_table.sort_values(perf_table.columns[0], inplace=True)\n","    print(perf_table)\n","    return perf_table\n","def powspace(start, stop, power, num):\n","    start = np.power(start, 1/float(power))\n","    stop = np.power(stop, 1/float(power))\n","    return np.power(np.linspace(start, stop, num=num), power)\n","def xgb_custom_lossfunction(alpha = 1):\n","    def support_under_mse(label, pred):\n","        # grad : 1차 미분\n","        # hess : 2차 미분\n","        residual = (label - pred).astype(\"float\")\n","        grad = np.where(residual > 0, -2 * alpha * residual, -2 * residual)\n","        hess = np.where(residual > 0, 2 * alpha, 2.0)\n","        return grad, hess\n","    return support_under_mse\n","def pd_flatten(df):\n","    df = df.unstack()\n","    df.index = [str(i) + \"_\" + str(j) for i, j in df.index]\n","    return df\n","def tf_losses_rmse(y_true, y_pred, sample_weight=None):\n","    return tf.sqrt(tf.reduce_mean((y_true - y_pred) ** 2)) if sample_weight is None else tf.sqrt(tf.reduce_mean(((y_true - y_pred) ** 2) * sample_weight))\n","def tf_loss_nmae(y_true, y_pred, sample_weight=False):\n","    mae = tf.reduce_mean(tf.math.abs(y_true - y_pred))\n","    score = tf.math.divide(mae, tf.reduce_mean(tf.math.abs(y_true)))\n","    return score\n","def text_extractor(string, lang=\"eng\", spacing=True):\n","    # # 괄호를 포함한 괄호 안 문자 제거 정규식\n","    # re.sub(r'\\([^)]*\\)', '', remove_text)\n","    # # <>를 포함한 <> 안 문자 제거 정규식\n","    # re.sub(r'\\<[^)]*\\>', '', remove_text)\n","    if lang == \"eng\":\n","        text_finder = re.compile('[^ A-Za-z]') if spacing else re.compile('[^A-Za-z]')\n","    elif lang == \"kor\":\n","        text_finder = re.compile('[^ ㄱ-ㅣ가-힣+]') if spacing else re.compile('[^ㄱ-ㅣ가-힣+]')\n","    # default : kor + eng\n","    else:\n","        text_finder = re.compile('[^ A-Za-zㄱ-ㅣ가-힣+]') if spacing else re.compile('[^A-Za-zㄱ-ㅣ가-힣+]')\n","    return text_finder.sub('', string)\n","def memory_usage(message='debug'):\n","    # current process RAM usage\n","    p = psutil.Process()\n","    rss = p.memory_info().rss / 2 ** 20 # Bytes to MB\n","    print(f\"[{message}] memory usage: {rss: 10.3f} MB\")\n","    return rss\n","def cos_sim(a, b):\n","    return dot(a, b)/(norm(a) * norm(b))\n","class MyLabelEncoder:\n","    def __init__(self, preset={}):\n","        # dic_cat format -> {\"col_name\": {\"value\": replace}}\n","        self.dic_cat = preset\n","    def fit_transform(self, data_x, col_names):\n","        tmp_x = copy.deepcopy(data_x)\n","        for i in col_names:\n","            # if key is not in dic, update dic\n","            if i not in self.dic_cat.keys():\n","                tmp_dic = dict.fromkeys(sorted(set(tmp_x[i]).difference([nan])))\n","                label_cnt = 0\n","                for j in tmp_dic.keys():\n","                    tmp_dic[j] = label_cnt\n","                    label_cnt += 1\n","                self.dic_cat[i] = tmp_dic\n","            # transform value which is not in dic to nan\n","            tmp_x[i] = tmp_x[i].astype(\"object\")\n","            conv = tmp_x[i].replace(self.dic_cat[i])\n","            for conv_idx, j in enumerate(conv):\n","                if j not in self.dic_cat[i].values():\n","                    conv[conv_idx] = nan\n","            # final return\n","            tmp_x[i] = conv.astype(\"float\")\n","        return tmp_x\n","    def transform(self, data_x):\n","        tmp_x = copy.deepcopy(data_x)\n","        for i in self.dic_cat.keys():\n","            # transform value which is not in dic to nan\n","            tmp_x[i] = tmp_x[i].astype(\"object\")\n","            conv = tmp_x[i].replace(self.dic_cat[i])\n","            for conv_idx, j in enumerate(conv):\n","                if j not in self.dic_cat[i].values():\n","                    conv[conv_idx] = nan\n","            # final return\n","            tmp_x[i] = conv.astype(\"float\")\n","        return tmp_x\n","    def clear(self):\n","        self.dic_cat = {}\n","class MyOneHotEncoder:\n","    def __init__(self, label_preset={}):\n","        self.dic_cat = {}\n","        self.label_preset = label_preset\n","    def fit_transform(self, data_x, col_names):\n","        tmp_x = dataframe()\n","        for i in data_x:\n","            if i not in col_names:\n","                tmp_x = pd.concat([tmp_x, dataframe(data_x[i])], axis=1)\n","            else:\n","                if not ((data_x[i].dtype.name == \"object\") or (data_x[i].dtype.name == \"category\")):\n","                    print(F\"WARNING : {i} is not object or category\")\n","                self.dic_cat[i] = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n","                conv = self.dic_cat[i].fit_transform(dataframe(data_x[i])).astype(\"int\")\n","                col_list = []\n","                for j in self.dic_cat[i].categories_[0]:\n","                    if i in self.label_preset.keys():\n","                        for k, v in self.label_preset[i].items():\n","                            if v == j:\n","                                col_list.append(str(i) + \"_\" + str(k))\n","                    else:\n","                        col_list.append(str(i) + \"_\" + str(j))\n","                conv = dataframe(conv, columns=col_list)\n","                tmp_x = pd.concat([tmp_x, conv], axis=1)\n","        return tmp_x\n","    def transform(self, data_x):\n","        tmp_x = dataframe()\n","        for i in data_x:\n","            if not i in list(self.dic_cat.keys()):\n","                tmp_x = pd.concat([tmp_x, dataframe(data_x[i])], axis=1)\n","            else:\n","                if not ((data_x[i].dtype.name == \"object\") or (data_x[i].dtype.name == \"category\")):\n","                    print(F\"WARNING : {i} is not object or category\")\n","                conv = self.dic_cat[i].transform(dataframe(data_x[i])).astype(\"int\")\n","                col_list = []\n","                for j in self.dic_cat[i].categories_[0]:\n","                    if i in self.label_preset.keys():\n","                        for k, v in self.label_preset[i].items():\n","                            if v == j: col_list.append(str(i) + \"_\" + str(k))\n","                    else:\n","                        col_list.append(str(i) + \"_\" + str(j))\n","                conv = dataframe(conv, columns=col_list)\n","                tmp_x = pd.concat([tmp_x, conv], axis=1)\n","        return tmp_x\n","    def clear(self):\n","        self.dic_cat = {}\n","        self.label_preset = {}\n","class MyKNNImputer:\n","    def __init__(self, k=5):\n","        self.imputer = KNNImputer(n_neighbors=k)\n","        self.dic_cat = {}\n","    def fit_transform(self, x, cat_vars=None):\n","        if cat_vars is None:\n","            x_imp = dataframe(self.imputer.fit_transform(x), columns=x.columns)\n","        else:\n","            naIdx = dict.fromkeys(cat_vars)\n","            for i in cat_vars:\n","                self.dic_cat[i] = diff(list(sorted(set(x[i]))), [nan])\n","                naIdx[i] = list(which(array(x[i].isna())))\n","            x_imp = dataframe(self.imputer.fit_transform(x), columns=x.columns)\n","\n","            # if imputed categorical value are not in the range, adjust the value\n","            for i in cat_vars:\n","                x_imp[i] = x_imp[i].apply(lambda x: int(round(x, 0)))\n","                for j in naIdx[i]:\n","                    if x_imp[i][j] not in self.dic_cat[i]:\n","                        if x_imp[i][j] < self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][0]\n","                        elif x_imp[i][j] > self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][len(self.dic_cat[i]) - 1]\n","        return x_imp\n","    def transform(self, x):\n","        if len(self.dic_cat.keys()) == 0:\n","            x_imp = dataframe(self.imputer.transform(x), columns=x.columns)\n","        else:\n","            naIdx = dict.fromkeys(self.dic_cat.keys())\n","            for i in self.dic_cat.keys():\n","                naIdx[i] = list(which(array(x[i].isna())))\n","            x_imp = dataframe(self.imputer.transform(x), columns=x.columns)\n","\n","            # if imputed categorical value are not in the range, adjust the value\n","            for i in self.dic_cat.keys():\n","                x_imp[i] = x_imp[i].apply(lambda x: int(round(x, 0)))\n","                for j in naIdx[i]:\n","                    if x_imp[i][j] not in self.dic_cat[i]:\n","                        if x_imp[i][j] < self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][0]\n","                        elif x_imp[i][j] > self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][len(self.dic_cat[i]) - 1]\n","        return x_imp\n","    def clear(self):\n","        self.imputer = None\n","        self.dic_cat = {}\n","def remove_outlier(df, std=3, mode=\"remove\"):\n","    tmp_df = df.copy()\n","    if mode == \"remove\":\n","        outlier_mask = (np.abs(stats.zscore(tmp_df)) > std).all(axis=1)\n","        print(\"found outlier :\", outlier_mask.sum())\n","        tmp_df = tmp_df[~outlier_mask]\n","    elif mode == \"interpolate\":\n","        tmp_outlier = []\n","        for i in tmp_df:\n","            outlier_mask = (np.abs(stats.zscore(tmp_df[i])) > std)\n","            tmp_outlier.append(outlier_mask.sum())\n","            if tmp_outlier[-1] == 0:\n","                continue\n","            tmp_df[i][outlier_mask] = np.nan\n","            tmp_df[i] = tmp_df[i].interpolate(method='linear').bfill()\n","        print(\"found outlier :\", np.sum(outlier_mask))\n","    return tmp_df\n","def convert_sparse_matrix_to_sparse_tensor(X, sorted=True):\n","    coo = X.tocoo()\n","    indices = np.mat([coo.row, coo.col]).transpose()\n","    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape)) if sorted else tf.SparseTensor(indices, coo.data, coo.shape)\n","def tfds_to_df(x):\n","    metadata = x.element_spec\n","    if type(metadata) is tuple:\n","        tmp = dataframe(columns=range(len(metadata)), index=range(x.cardinality().numpy()))\n","        for idx, value in enumerate(x.as_numpy_iterator()):\n","            tmp.iloc[idx] = value\n","    elif type(metadata) is dict:\n","        tmp = dataframe(columns=list(metadata.keys()), index=range(x.cardinality().numpy()))\n","        for idx, value in enumerate(x.as_numpy_iterator()):\n","            tmp.iloc[idx] = value\n","    # if single tensor\n","    else:\n","        tmp = series(index=range(x.cardinality().numpy()))\n","        for idx, value in enumerate(x.as_numpy_iterator()):\n","            tmp.iloc[idx] = value\n","    return tmp"],"metadata":{"id":"shE4IUQjpSw_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data loading"],"metadata":{"id":"3MMMkKOkpRlk"}},{"cell_type":"code","source":["folder_path = \"./dacon/LG_selfdriving_antenna_performance_prediction/\"\n","feature_version = 10\n","\n","allTarget = range(1,15,1)\n","allTarget = [\"Y_\" + str(i).zfill(2) for i in allTarget]\n","\n","df_full = pd.read_csv(folder_path + \"rawdata/train.csv\")\n","df_test_x = pd.read_csv(folder_path + \"rawdata/test.csv\")\n","df_full_info = pd.read_csv(folder_path + \"rawdata/meta/x_feature_info_v2.csv\")\n","\n","num_vars = df_full_info[\"Feature\"][df_full_info[\"타입\"] == \"numeric\"].to_list()\n","bin_vars = df_full_info[\"Feature\"][df_full_info[\"타입\"] == \"binary\"].to_list()\n","feature_info = {\n","    \"num_vars\": num_vars,\n","    \"bin_vars\": bin_vars\n","}\n","\n","df_full_x = df_full.filter(regex=\"X\")\n","df_full_y = df_full.filter(regex=\"Y\")\n","del df_full"],"metadata":{"id":"GzBdOye2pSAH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"4ook9gWspZTZ"}},{"cell_type":"code","source":["class GetSequenceInteraction():\n","    def __init__(self, idx_var=None, process_interaction=3, feature_scale_vars=[], drop_vars=[]):\n","        self.idx_var = None\n","        self.process_interaction = process_interaction\n","        self.feature_scale_vars = feature_scale_vars\n","        self.drop_vars = drop_vars\n","        self.scaler = None\n","\n","    def fit(self, x):\n","        x = x.reset_index(drop=True)\n","        # 모든 값이 같은, 즉 분산이 0인 feature 제거 -> 검사통과여부 관련\n","        if len(self.drop_vars) > 0:\n","            x = self.drop_var_zero(x)\n","        # N개 까지의 feature interaction feature 추가\n","        self.scaler = ColumnTransformer(\n","            [(\"numerical_std_scaler\", StandardScaler(), self.feature_scale_vars)]\n","        )\n","        self.scaler.fit(x)\n","        x = self.feature_sequence_interaction(x)\n","        return self\n","\n","    def transform(self, x):\n","        x = x.reset_index(drop=True)\n","        if len(self.drop_vars) > 0:\n","            x = self.drop_var_zero(x)\n","        x = self.feature_sequence_interaction(x)\n","        return x\n","\n","    def drop_var_zero(self, x):\n","        x_copy = copy.deepcopy(x)\n","        x_copy = x_copy.drop(list(set(list(x_copy.columns[x_copy.var() == 0]) + self.drop_vars)), axis=1)\n","        return x_copy\n","\n","    def feature_sequence_interaction(self, x):\n","        x_copy = dataframe(index=x.index.values)\n","        cols = list(x.columns)\n","        for i in range(x.shape[1]):\n","            if i >= self.process_interaction - 1:\n","                tmp1 = dataframe(self.scaler.transform(x), columns=cols).iloc[:, (i + 1 - self.process_interaction):(i + 1)]\n","                # tmp1 = dataframe(array(x), columns=cols).iloc[:, (i+1-self.process_interaction):(i+1)]\n","                tmp2 = tmp1.prod(axis=1)\n","                tmp2.name = \"_\".join(list(tmp1.columns))\n","                x_copy = pd.concat([x_copy, tmp2.to_frame()], axis=1)\n","        return x_copy\n","\n","\n","class feature_engineering_v1():\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, x):\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","        return x_copy\n","\n","\n","class feature_engineering_v2():\n","    def __init__(self):\n","        self.feature_scale_vars = None\n","        self.drop_vars = [\"X_04\", \"X_23\", \"X_47\", \"X_48\"] + [\"X_10\", \"X_11\"] + [\"X_02\"]\n","        # self.scaler = None\n","        self.fe = []\n","\n","    def fit(self, x):\n","        x_copy = copy.deepcopy(x)\n","        # N개 까지의 feature interaction feature 추가\n","        self.feature_scale_vars = diff(x_copy.columns, self.drop_vars)\n","        self.fe.append(\n","            GetSequenceInteraction(process_interaction=2, feature_scale_vars=self.feature_scale_vars, drop_vars=[i for i in x_copy.columns if i in self.drop_vars]).fit(x_copy))\n","        self.fe.append(\n","            GetSequenceInteraction(process_interaction=3, feature_scale_vars=self.feature_scale_vars, drop_vars=[i for i in x_copy.columns if i in self.drop_vars]).fit(x_copy))\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","        x_copy = pd.concat(\n","            [x_copy.drop([i for i in x_copy.columns if i in self.drop_vars], axis=1), self.fe[0].transform(x_copy), self.fe[1].transform(x_copy)],\n","            axis=1\n","        )\n","        return x_copy\n","\n","\n","class feature_engineering_v3():\n","    def __init__(self):\n","        self.drop_vars = [\"X_04\", \"X_23\", \"X_47\", \"X_48\"] + [\"X_10\", \"X_11\"] + [\"X_02\"]\n","\n","    def fit(self, x):\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","        return x_copy.drop(self.drop_vars, axis=1)\n","\n","\n","class feature_engineering_v4():\n","    def __init__(self):\n","        self.drop_vars = [\"X_04\", \"X_23\", \"X_47\", \"X_48\"] + [\"X_10\", \"X_11\"] + [\"X_02\"]\n","        self.scaler = StandardScaler()\n","        self.pf = PolynomialFeatures(interaction_only=False, include_bias=False)\n","\n","    def fit(self, x):\n","        x_copy = copy.deepcopy(x)\n","        x_copy = x_copy.drop(self.drop_vars, axis=1)\n","        x_copy = self.scaler.fit_transform(x_copy)\n","        self.pf.fit(x_copy)\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","        x_copy = x_copy.drop(self.drop_vars, axis=1)\n","        x_copy = self.scaler.transform(x_copy)\n","        x_copy = self.pf.transform(x_copy)\n","        return dataframe(x_copy)\n","\n","\n","class feature_engineering_v5():\n","    def __init__(self):\n","        self.drop_vars = [\"X_04\", \"X_23\", \"X_47\", \"X_48\"] + [\"X_10\", \"X_11\"]\n","        self.consider_as_process = [\n","            [\"X_01\", \"X_02\", \"X_05\", \"X_06\"],\n","            [\"X_14\", \"X_15\", \"X_16\", \"X_17\", \"X_18\"],\n","            [\"X_19\", \"X_20\", \"X_21\", \"X_22\"],\n","            [\"X_24\", \"X_25\", \"X_26\", \"X_27\", \"X_28\", \"X_29\"],\n","            [\"X_30\", \"X_31\", \"X_32\", \"X_33\"],\n","            [\"X_34\", \"X_35\", \"X_36\", \"X_37\"],\n","            [\"X_38\", \"X_39\", \"X_40\"],\n","            [\"X_41\", \"X_42\", \"X_43\", \"X_44\"],\n","            [\"X_50\", \"X_51\", \"X_52\", \"X_53\", \"X_54\", \"X_55\", \"X_56\"],\n","        ]\n","\n","    def fit(self, x):\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","        x_copy = x_copy.drop(self.drop_vars, axis=1)\n","\n","        for idx, value in enumerate(self.consider_as_process):\n","            x_copy[\"process_\" + str(idx) + \"_mean\"] = x_copy[value].mean(axis=1)\n","            x_copy[\"process_\" + str(idx) + \"_std\"] = x_copy[value].std(axis=1)\n","\n","        return x_copy\n","\n","\n","class feature_engineering_v6():\n","    def __init__(self):\n","        self.drop_vars = [\"X_04\", \"X_23\", \"X_47\", \"X_48\"] + [\"X_10\", \"X_11\"] + [\"X_02\"]\n","\n","    def fit(self, x):\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","        return x_copy.drop(self.drop_vars, axis=1)\n","\n","\n","class feature_engineering_v7():\n","    def __init__(self):\n","        self.drop_vars = [\"X_04\", \"X_23\", \"X_47\", \"X_48\"] + [\"X_10\", \"X_11\"] + [\"X_02\"]\n","        self.consider_as_process = [\n","            [\"X_01\", \"X_05\", \"X_06\"],\n","            [\"X_14\", \"X_15\", \"X_16\", \"X_17\", \"X_18\"],\n","            [\"X_19\", \"X_20\", \"X_21\", \"X_22\"],\n","            [\"X_24\", \"X_25\", \"X_26\", \"X_27\", \"X_28\", \"X_29\"],\n","            [\"X_30\", \"X_31\", \"X_32\", \"X_33\"],\n","            [\"X_34\", \"X_35\", \"X_36\", \"X_37\"],\n","            [\"X_38\", \"X_39\", \"X_40\"],\n","            [\"X_41\", \"X_42\", \"X_43\", \"X_44\"],\n","            [\"X_50\", \"X_51\", \"X_52\", \"X_53\", \"X_54\", \"X_55\", \"X_56\"],\n","        ]\n","\n","    def fit(self, x):\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","\n","        for idx, value in enumerate(self.consider_as_process):\n","            x_copy[\"process_\" + str(idx) + \"_mean\"] = x_copy[value].mean(axis=1)\n","            x_copy[\"process_\" + str(idx) + \"_std\"] = x_copy[value].std(axis=1)\n","\n","        x_copy = x_copy.drop(self.drop_vars, axis=1)\n","        return x_copy\n","\n","\n","class feature_engineering_v8():\n","    def __init__(self):\n","        self.fe4 = feature_engineering_v4()\n","        self.fe7 = feature_engineering_v7()\n","\n","    def fit(self, x):\n","        x_copy = copy.deepcopy(x)\n","        self.fe4.fit(x_copy)\n","        self.fe7.fit(x_copy)\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","\n","        fe4_x = self.fe4.transform(x_copy)\n","        fe7_x = self.fe7.transform(x_copy)\n","\n","        x_copy = pd.concat([fe4_x, fe7_x], axis=1, ignore_index=True)\n","        return x_copy\n","\n","\n","class feature_engineering_v9():\n","    def __init__(self):\n","        self.fe4 = feature_engineering_v4()\n","        self.fe7 = feature_engineering_v7()\n","        self.fe2 = feature_engineering_v2()\n","\n","    def fit(self, x):\n","        x_copy = copy.deepcopy(x)\n","\n","        self.fe4.fit(x_copy)\n","        self.fe7.fit(x_copy)\n","        x_copy = self.fe7.transform(x_copy)\n","        self.fe2.fit(x_copy.filter(regex=\"mean\"))\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","\n","        fe4_x = self.fe4.transform(x_copy)\n","        fe7_x = self.fe7.transform(x_copy)\n","        fe2_x = self.fe2.transform(fe7_x.filter(regex=\"mean\"))\n","\n","        x_copy = pd.concat([fe7_x, fe4_x.iloc[:, 49:], fe2_x.iloc[:, 9:]], axis=1, ignore_index=True)\n","        return x_copy\n","\n","\n","class feature_engineering_v10():\n","    def __init__(self):\n","        self.fe7 = feature_engineering_v7()\n","\n","    def fit(self, x):\n","        x_copy = copy.deepcopy(x)\n","\n","        self.fe7.fit(x_copy)\n","        return self\n","\n","    def transform(self, x):\n","        x_copy = copy.deepcopy(x)\n","        fe7_x = self.fe7.transform(x_copy)\n","        x_copy = fe7_x\n","        return x_copy\n","\n","fe = feature_engineering_v3()\n","fe.fit(df_full_x)\n","df_full_x = fe.transform(df_full_x)\n","df_test_x = fe.transform(df_test_x.drop(\"ID\", axis=1))\n","\n","np_rnd.seed(42)\n","shuffled_idx = np_rnd.permutation(len(df_full_x))\n","df_full_x = df_full_x.iloc[shuffled_idx, :].reset_index(drop=True)\n","df_full_y = df_full_y.iloc[shuffled_idx, :].reset_index(drop=True)"],"metadata":{"id":"jZaXE9D5pSdo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training & Inference - H2O Automl"],"metadata":{"id":"86AhXLVlpyYO"}},{"cell_type":"code","source":["h2o.init(nthreads=-1, max_mem_size=8)\n","\n","seed_everything()\n","for cnt, value in enumerate(allTarget):\n","    tmp_df = h2o.H2OFrame(pd.concat([df_full_x.astype(\"float32\"), df_full_y[[value]].astype(\"float32\")], axis=1))\n","    feature_names = list(df_full_x.columns)\n","    target_name = value\n","\n","    aml = h2o.automl.H2OAutoML(max_runtime_secs=int(3600 * 24 / len(allTarget)), seed=42, project_name=\"LG_selfDriving_automl_\" + value + \"_try1\",\n","                               nfolds=5, stopping_metric=\"MAE\", stopping_rounds=100)\n","    aml.train(training_frame=tmp_df, x=feature_names, y=target_name)\n","    print(aml.leaderboard.head())\n","    inference_root_path = folder_path + \"inference/\" + \"featureV\" + str(feature_version) + \"+\" + \"H2OAutoML\" + \"/\"\n","    createFolder(inference_root_path)\n","    if os.path.exists(inference_root_path + \"submission_\" + \"featureV\" + str(feature_version) + \"+\" + \"H2OAutoML\" + \".csv\"):\n","        submission = pd.read_csv(inference_root_path + \"submission_\" + \"featureV\" + str(feature_version) + \"+\" + \"H2OAutoML\" + \".csv\")\n","    else:\n","        submission = pd.read_csv(folder_path + 'rawdata/sample_submission.csv')\n","    submission[value] = aml.predict(h2o.H2OFrame(df_test_x.astype(\"float32\"))).as_data_frame().to_numpy().flatten()\n","    submission.to_csv(inference_root_path + \"submission_\" + \"featureV\" + str(feature_version) + \"+\" + \"H2OAutoML\" + \".csv\", index=False)\n","\n","h2o.cluster().shutdown()"],"metadata":{"id":"AmKp5SEepSbp"},"execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"main_h2o_github.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}